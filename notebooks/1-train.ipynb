{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from copy import deepcopy\n",
    "from functools import partial\n",
    "from itertools import combinations\n",
    "import gc\n",
    "\n",
    "## sklearn imports\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split,\n",
    "    StratifiedKFold,\n",
    "    KFold\n",
    ")\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import ensemble\n",
    "from category_encoders import (\n",
    "    OneHotEncoder,\n",
    "    OrdinalEncoder,\n",
    "    CountEncoder,\n",
    "    CatBoostEncoder\n",
    ")\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "## hyperparameter optimizer\n",
    "import optuna\n",
    "\n",
    "## boosting libraries\n",
    "import xgboost as xgb\n",
    "from catboost import (\n",
    "    CatBoost,\n",
    "    CatBoostClassifier,\n",
    "    CatBoostRegressor\n",
    ")\n",
    "from catboost import Pool\n",
    "\n",
    "# supress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "\n",
    "# configure pandas\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "df_train = pd.read_csv(\"../data/train.csv\")\n",
    "df_test = pd.read_csv(\"../data/test.csv\")\n",
    "\n",
    "target_col = 'Class'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape :(117564, 9) , y_train shape :(117564,)\n",
      "X_test shape :(78377, 9)\n"
     ]
    }
   ],
   "source": [
    "# prepare train and test data\n",
    "X_train = df_train.drop([f'{target_col}'], axis=1).reset_index(drop=True)\n",
    "y_train = df_train[f'{target_col}'].reset_index(drop=True)\n",
    "X_test = df_test.reset_index(drop=True)\n",
    "\n",
    "print(f\"X_train shape :{X_train.shape} , y_train shape :{y_train.shape}\")\n",
    "print(f\"X_test shape :{X_test.shape}\")\n",
    "\n",
    "# Delete the train and test dataframes to free up memory\n",
    "del df_train, df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessor:\n",
    "    def __init__(self, numeric_columns=None, max_pattern=2):\n",
    "        self.numeric_columns = numeric_columns\n",
    "        self.max_pattern = max_pattern\n",
    "        self.scaler = None\n",
    "\n",
    "    def preprocess(self, X_train, X_test):\n",
    "        X_train = self.create_numeric_combinations(X_train)\n",
    "        X_test = self.create_numeric_combinations(X_test)\n",
    "\n",
    "        numeric_columns = [_ for _ in X_train.columns if X_train[_].dtype=='float']\n",
    "        scaler = StandardScaler()\n",
    "        X_train[numeric_columns] = scaler.fit_transform(X_train[numeric_columns])\n",
    "        X_test[numeric_columns] = scaler.transform(X_test[numeric_columns])\n",
    "\n",
    "        return X_train, X_test\n",
    "    \n",
    "    def create_numeric_combinations(self, df):\n",
    "        new_cols = []\n",
    "        for comb in range(2, len(self.numeric_columns) + 1):\n",
    "            for col in combinations(self.numeric_columns, comb):\n",
    "                if len(col) > self.max_pattern:\n",
    "                    break\n",
    "                col_names = list(col)\n",
    "                new_col = '_'.join(col_names) + '_mult'\n",
    "                df[new_col] = df[col_names[0]]\n",
    "                for c in col_names[1:]:\n",
    "                    df[new_col] *= df[c]\n",
    "                new_cols.append(new_col)\n",
    "\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape :(117564, 45) X_test shape :(78377, 45)\n"
     ]
    }
   ],
   "source": [
    "numeric_columns = [_ for _ in X_test.columns if 'is_generated' not in _]\n",
    "pp = Preprocessor(numeric_columns=numeric_columns)\n",
    "X_train, X_test = pp.preprocess(X_train=X_train, X_test=X_test)\n",
    "print(f\"X_train shape :{X_train.shape}\", f\"X_test shape :{X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Splitter:\n",
    "    def __init__(self, test_size=0.2, kfold=True, n_splits=5):\n",
    "        self.test_size = test_size\n",
    "        self.kfold = kfold\n",
    "        self.n_splits = n_splits\n",
    "\n",
    "    def split_data(self, X, y, random_state_list):\n",
    "        if self.kfold:\n",
    "            for random_state in random_state_list:\n",
    "                kf = StratifiedKFold(\n",
    "                    n_splits=self.n_splits,\n",
    "                    random_state=random_state,\n",
    "                    shuffle=True\n",
    "                )\n",
    "                for train_index, val_index in kf.split(X, y):\n",
    "                    X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n",
    "                    y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n",
    "                    yield X_train, X_val, y_train, y_val\n",
    "        else:\n",
    "                for random_state in random_state_list:\n",
    "                    X_train, X_val, y_train, y_val = train_test_split(\n",
    "                        X,\n",
    "                        y, \n",
    "                        test_size=self.test_size, \n",
    "                        random_state=random_state\n",
    "                    )\n",
    "                    yield X_train, X_val, y_train, y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#random.randint(1000,9999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = True\n",
    "n_splits = 10\n",
    "random_state = 317\n",
    "random_state_list = [1792, 7145, 7237]\n",
    "n_estimators = 9999\n",
    "early_stopping_rounds = 100\n",
    "verbose = False\n",
    "device = 'cpu'\n",
    "\n",
    "splitter = Splitter(kfold=kfold, n_splits=n_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StackingClassifier:\n",
    "    def __init__(self, n_estimators=100, device='cpu', random_state=0):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.device = device\n",
    "        self.random_state = random_state\n",
    "        self.base_models = self._define_base_models()\n",
    "        self.meta_models = self._define_class_models()\n",
    "        self.len_base = len(self.base_models)\n",
    "        self.len_meta = len(self.meta_models)\n",
    "\n",
    "    def _define_class_models(self,):\n",
    "        xgb_params = {\n",
    "            'n_estimators': self.n_estimators,\n",
    "            'max_depth': 4,\n",
    "            'learning_rate': 0.06333221939055333,\n",
    "            'min_child_weight': 4,\n",
    "            'gamma': 5.301218558776368e-08,\n",
    "            'subsample': 0.41010429946197946,\n",
    "            'colsample_bytree': 0.8298539920447499,\n",
    "            'reg_alpha': 0.000517878113716743,\n",
    "            'reg_lambda': 0.00030121415155097723,\n",
    "            'n_jobs': -1,\n",
    "            'objective': 'binary:logistic',\n",
    "            'verbosity': 0,\n",
    "            'eval_metric': 'logloss',\n",
    "            'random_state': self.random_state\n",
    "        }\n",
    "        if self.device == 'gpu':\n",
    "            xgb_params['tree_method'] = 'gpu_hist'\n",
    "            xgb_params['predictor'] = 'gpu_predictor'\n",
    "        \n",
    "        cb_params = {\n",
    "            'iterations': self.n_estimators,\n",
    "            'depth': 3,\n",
    "            'learning_rate': 0.15687380686250746,\n",
    "            'l2_leaf_reg': 4.0368544113430485,\n",
    "            'random_strength': 0.1279482215776108,\n",
    "            'max_bin': 238,\n",
    "            'od_wait': 49,\n",
    "            'one_hot_max_size': 39,\n",
    "            'grow_policy': 'SymmetricTree',\n",
    "            'bootstrap_type': 'Bayesian',\n",
    "            'od_type': 'Iter',\n",
    "            'loss_function': 'Logloss',\n",
    "            'task_type': self.device.upper(),\n",
    "            'random_state': self.random_state\n",
    "        }\n",
    "\n",
    "        class_models = {\n",
    "            'xgb_class': xgb.XGBClassifier(**xgb_params),\n",
    "            'cat_class': CatBoostClassifier(**cb_params)\n",
    "        }\n",
    "\n",
    "        return class_models\n",
    "        \n",
    "    def _define_reg_model(self):\n",
    "\n",
    "        xgb_params = {\n",
    "            'n_estimators': self.n_estimators,\n",
    "            'max_depth': 4,\n",
    "            'learning_rate': 0.06604482627857397,\n",
    "            'min_child_weight': 9,\n",
    "            'gamma': 2.785627092225762e-06,\n",
    "            'subsample': 0.3600730418583202,\n",
    "            'colsample_bytree': 0.643296031751869,\n",
    "            'reg_alpha': 0.00048086062508489406,\n",
    "            'reg_lambda': 8.080844212784364e-06,\n",
    "            'n_jobs': -1,\n",
    "            'objective': 'reg:logistic',\n",
    "            'verbosity': 0,\n",
    "            'eval_metric': 'rmse',\n",
    "            'random_state': self.random_state\n",
    "        }\n",
    "        if self.device == 'gpu':\n",
    "            xgb_params['tree_method'] = 'gpu_hist'\n",
    "            xgb_params['predictor'] = 'gpu_predictor'\n",
    "        \n",
    "        cb_params = {\n",
    "            'iterations': self.n_estimators,\n",
    "            'depth': 5,\n",
    "            'learning_rate': 0.12947105266151432,\n",
    "            'l2_leaf_reg': 0.6169164517797081,\n",
    "            'random_strength': 0.21235850198764036,\n",
    "            'max_bin': 212,\n",
    "            'od_wait': 67,\n",
    "            'one_hot_max_size': 73,\n",
    "            'grow_policy': 'Depthwise',\n",
    "            'bootstrap_type': 'Bayesian',\n",
    "            'od_type': 'Iter',\n",
    "            'loss_function': 'RMSE',\n",
    "            'task_type': self.device.upper(),\n",
    "            'random_state': self.random_state\n",
    "        }\n",
    "\n",
    "        reg_models = {\n",
    "            'xgb_reg': xgb.XGBRegressor(**xgb_params),\n",
    "            'cat_reg': CatBoostRegressor(**cb_params)\n",
    "        }\n",
    "\n",
    "        return reg_models\n",
    "    \n",
    "    def _define_add_model(self):\n",
    "\n",
    "        add_models = {\n",
    "            'hgbc_class': ensemble.HistGradientBoostingClassifier(max_iter=500, max_depth=4, random_state=self.random_state),\n",
    "            'lr_class': LogisticRegression(max_iter=1000, n_jobs=-1),\n",
    "            'rf_class': ensemble.RandomForestClassifier(n_estimators=100, max_depth=4, random_state=self.random_state, n_jobs=-1)\n",
    "        }\n",
    "\n",
    "        return add_models\n",
    "    \n",
    "    def _define_base_models(self,):\n",
    "\n",
    "        class_models = self._define_class_models()\n",
    "        reg_models = self._define_reg_model()\n",
    "        add_models = self._define_add_model()\n",
    "        base_models = {\n",
    "            **class_models,\n",
    "            **reg_models,\n",
    "            **add_models\n",
    "        }\n",
    "\n",
    "        return base_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## define optuna optimizer for weights\n",
    "class OptunaWeights:\n",
    "\n",
    "    def __init__(self, random_state):\n",
    "\n",
    "        self.study = None\n",
    "        self.weights = None\n",
    "        self.random_state = random_state\n",
    "\n",
    "    def _objective(self, trial, y_true, y_preds):\n",
    "\n",
    "        # define weights for the predictions of each model\n",
    "        weights = [trial.suggest_float(f'weight{n}', 0, 1) for n in range(len(y_preds))]\n",
    "\n",
    "        # calculate the weighted prediction\n",
    "        weighted_pred = np.average(np.array(y_preds).T, axis=1, weights=weights)\n",
    "\n",
    "        # calculate the Logloss score for the weighted prediction\n",
    "        score = log_loss(y_true, weighted_pred)\n",
    "\n",
    "        return score\n",
    "    \n",
    "    def fit(self, y_true, y_preds, n_trials=300):\n",
    "\n",
    "        optuna.logging.set_verbosity(optuna.logging.ERROR)\n",
    "        sampler = optuna.samplers.CmaEsSampler(seed=self.random_state)\n",
    "        self.study = optuna.create_study(sampler=sampler, study_name=\"OptunaWeights\", direction='minimize')\n",
    "        objective_partial = partial(self._objective, y_true=y_true, y_preds=y_preds)\n",
    "        self.study.optimize(objective_partial, n_trials=n_trials)\n",
    "        self.weights = [self.study.best_params[f'weight{n}'] for n in range(len(y_preds))]\n",
    "\n",
    "    def predict(self, y_preds):\n",
    "        assert self.weights is not None, 'OptunaWeights error, must be fitted before predict'\n",
    "        weighted_pred = np.average(np.array(y_preds).T, axis=1, weights=self.weights)\n",
    "\n",
    "        return weighted_pred\n",
    "    \n",
    "    def fit_predict(self, y_true, y_preds, n_trials=300):\n",
    "        self.fit(y_true, y_preds, n_trials=n_trials)\n",
    "        return self.predict(y_preds)\n",
    "    \n",
    "    def weights(self):\n",
    "        return self.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base MODEL xgb_class [FOLD-0 SEED-1792] LogLoss score: 0.03320\n",
      "Base MODEL cat_class [FOLD-0 SEED-1792] LogLoss score: 0.03384\n",
      "Base MODEL xgb_reg [FOLD-0 SEED-1792] LogLoss score: 0.03362\n",
      "Base MODEL cat_reg [FOLD-0 SEED-1792] LogLoss score: 0.03487\n",
      "Base MODEL hgbc_class [FOLD-0 SEED-1792] LogLoss score: 0.03544\n",
      "Base MODEL lr_class [FOLD-0 SEED-1792] LogLoss score: 0.05491\n",
      "Base MODEL rf_class [FOLD-0 SEED-1792] LogLoss score: 0.03814\n",
      "Base MODEL xgb_class [FOLD-1 SEED-1792] LogLoss score: 0.02799\n",
      "Base MODEL cat_class [FOLD-1 SEED-1792] LogLoss score: 0.02860\n",
      "Base MODEL xgb_reg [FOLD-1 SEED-1792] LogLoss score: 0.02810\n",
      "Base MODEL cat_reg [FOLD-1 SEED-1792] LogLoss score: 0.02970\n",
      "Base MODEL hgbc_class [FOLD-1 SEED-1792] LogLoss score: 0.03005\n",
      "Base MODEL lr_class [FOLD-1 SEED-1792] LogLoss score: 0.03649\n",
      "Base MODEL rf_class [FOLD-1 SEED-1792] LogLoss score: 0.03239\n",
      "Base MODEL xgb_class [FOLD-2 SEED-1792] LogLoss score: 0.03102\n",
      "Base MODEL cat_class [FOLD-2 SEED-1792] LogLoss score: 0.03155\n",
      "Base MODEL xgb_reg [FOLD-2 SEED-1792] LogLoss score: 0.03104\n",
      "Base MODEL cat_reg [FOLD-2 SEED-1792] LogLoss score: 0.04402\n",
      "Base MODEL hgbc_class [FOLD-2 SEED-1792] LogLoss score: 0.03161\n",
      "Base MODEL lr_class [FOLD-2 SEED-1792] LogLoss score: 0.05025\n",
      "Base MODEL rf_class [FOLD-2 SEED-1792] LogLoss score: 0.03468\n",
      "Base MODEL xgb_class [FOLD-3 SEED-1792] LogLoss score: 0.03637\n",
      "Base MODEL cat_class [FOLD-3 SEED-1792] LogLoss score: 0.03725\n",
      "Base MODEL xgb_reg [FOLD-3 SEED-1792] LogLoss score: 0.03701\n",
      "Base MODEL cat_reg [FOLD-3 SEED-1792] LogLoss score: 0.05232\n",
      "Base MODEL hgbc_class [FOLD-3 SEED-1792] LogLoss score: 0.03790\n",
      "Base MODEL lr_class [FOLD-3 SEED-1792] LogLoss score: 0.04836\n",
      "Base MODEL rf_class [FOLD-3 SEED-1792] LogLoss score: 0.04199\n",
      "Base MODEL xgb_class [FOLD-4 SEED-1792] LogLoss score: 0.03515\n",
      "Base MODEL cat_class [FOLD-4 SEED-1792] LogLoss score: 0.03554\n",
      "Base MODEL xgb_reg [FOLD-4 SEED-1792] LogLoss score: 0.03534\n",
      "Base MODEL cat_reg [FOLD-4 SEED-1792] LogLoss score: 0.04145\n",
      "Base MODEL hgbc_class [FOLD-4 SEED-1792] LogLoss score: 0.03648\n",
      "Base MODEL lr_class [FOLD-4 SEED-1792] LogLoss score: 0.04244\n",
      "Base MODEL rf_class [FOLD-4 SEED-1792] LogLoss score: 0.04038\n",
      "Base MODEL xgb_class [FOLD-5 SEED-1792] LogLoss score: 0.03107\n",
      "Base MODEL cat_class [FOLD-5 SEED-1792] LogLoss score: 0.03156\n",
      "Base MODEL xgb_reg [FOLD-5 SEED-1792] LogLoss score: 0.03166\n",
      "Base MODEL cat_reg [FOLD-5 SEED-1792] LogLoss score: 0.04245\n",
      "Base MODEL hgbc_class [FOLD-5 SEED-1792] LogLoss score: 0.03183\n",
      "Base MODEL lr_class [FOLD-5 SEED-1792] LogLoss score: 0.05029\n",
      "Base MODEL rf_class [FOLD-5 SEED-1792] LogLoss score: 0.03587\n",
      "Base MODEL xgb_class [FOLD-6 SEED-1792] LogLoss score: 0.03268\n",
      "Base MODEL cat_class [FOLD-6 SEED-1792] LogLoss score: 0.03278\n",
      "Base MODEL xgb_reg [FOLD-6 SEED-1792] LogLoss score: 0.03224\n",
      "Base MODEL cat_reg [FOLD-6 SEED-1792] LogLoss score: 0.03354\n",
      "Base MODEL hgbc_class [FOLD-6 SEED-1792] LogLoss score: 0.03373\n",
      "Base MODEL lr_class [FOLD-6 SEED-1792] LogLoss score: 0.03983\n",
      "Base MODEL rf_class [FOLD-6 SEED-1792] LogLoss score: 0.03555\n",
      "Base MODEL xgb_class [FOLD-7 SEED-1792] LogLoss score: 0.02964\n",
      "Base MODEL cat_class [FOLD-7 SEED-1792] LogLoss score: 0.02979\n",
      "Base MODEL xgb_reg [FOLD-7 SEED-1792] LogLoss score: 0.03028\n",
      "Base MODEL cat_reg [FOLD-7 SEED-1792] LogLoss score: 0.03037\n",
      "Base MODEL hgbc_class [FOLD-7 SEED-1792] LogLoss score: 0.03011\n",
      "Base MODEL lr_class [FOLD-7 SEED-1792] LogLoss score: 0.04402\n",
      "Base MODEL rf_class [FOLD-7 SEED-1792] LogLoss score: 0.03398\n",
      "Base MODEL xgb_class [FOLD-8 SEED-1792] LogLoss score: 0.02557\n",
      "Base MODEL cat_class [FOLD-8 SEED-1792] LogLoss score: 0.02562\n",
      "Base MODEL xgb_reg [FOLD-8 SEED-1792] LogLoss score: 0.02579\n",
      "Base MODEL cat_reg [FOLD-8 SEED-1792] LogLoss score: 0.02861\n",
      "Base MODEL hgbc_class [FOLD-8 SEED-1792] LogLoss score: 0.02758\n",
      "Base MODEL lr_class [FOLD-8 SEED-1792] LogLoss score: 0.04025\n",
      "Base MODEL rf_class [FOLD-8 SEED-1792] LogLoss score: 0.02887\n",
      "Base MODEL xgb_class [FOLD-9 SEED-1792] LogLoss score: 0.02490\n",
      "Base MODEL cat_class [FOLD-9 SEED-1792] LogLoss score: 0.02557\n",
      "Base MODEL xgb_reg [FOLD-9 SEED-1792] LogLoss score: 0.02524\n",
      "Base MODEL cat_reg [FOLD-9 SEED-1792] LogLoss score: 0.02847\n",
      "Base MODEL hgbc_class [FOLD-9 SEED-1792] LogLoss score: 0.02627\n",
      "Base MODEL lr_class [FOLD-9 SEED-1792] LogLoss score: 0.04484\n",
      "Base MODEL rf_class [FOLD-9 SEED-1792] LogLoss score: 0.02912\n",
      "Base MODEL xgb_class [FOLD-0 SEED-7145] LogLoss score: 0.03097\n",
      "Base MODEL cat_class [FOLD-0 SEED-7145] LogLoss score: 0.03134\n",
      "Base MODEL xgb_reg [FOLD-0 SEED-7145] LogLoss score: 0.03128\n",
      "Base MODEL cat_reg [FOLD-0 SEED-7145] LogLoss score: 0.03492\n",
      "Base MODEL hgbc_class [FOLD-0 SEED-7145] LogLoss score: 0.03179\n",
      "Base MODEL lr_class [FOLD-0 SEED-7145] LogLoss score: 0.04072\n",
      "Base MODEL rf_class [FOLD-0 SEED-7145] LogLoss score: 0.03445\n",
      "Base MODEL xgb_class [FOLD-1 SEED-7145] LogLoss score: 0.03000\n",
      "Base MODEL cat_class [FOLD-1 SEED-7145] LogLoss score: 0.03063\n",
      "Base MODEL xgb_reg [FOLD-1 SEED-7145] LogLoss score: 0.03052\n",
      "Base MODEL cat_reg [FOLD-1 SEED-7145] LogLoss score: 0.04441\n",
      "Base MODEL hgbc_class [FOLD-1 SEED-7145] LogLoss score: 0.03218\n",
      "Base MODEL lr_class [FOLD-1 SEED-7145] LogLoss score: 0.04814\n",
      "Base MODEL rf_class [FOLD-1 SEED-7145] LogLoss score: 0.03637\n",
      "Base MODEL xgb_class [FOLD-2 SEED-7145] LogLoss score: 0.03391\n",
      "Base MODEL cat_class [FOLD-2 SEED-7145] LogLoss score: 0.03435\n",
      "Base MODEL xgb_reg [FOLD-2 SEED-7145] LogLoss score: 0.03422\n",
      "Base MODEL cat_reg [FOLD-2 SEED-7145] LogLoss score: 0.03462\n",
      "Base MODEL hgbc_class [FOLD-2 SEED-7145] LogLoss score: 0.03486\n",
      "Base MODEL lr_class [FOLD-2 SEED-7145] LogLoss score: 0.04829\n",
      "Base MODEL rf_class [FOLD-2 SEED-7145] LogLoss score: 0.03781\n",
      "Base MODEL xgb_class [FOLD-3 SEED-7145] LogLoss score: 0.03198\n",
      "Base MODEL cat_class [FOLD-3 SEED-7145] LogLoss score: 0.03193\n",
      "Base MODEL xgb_reg [FOLD-3 SEED-7145] LogLoss score: 0.03202\n",
      "Base MODEL cat_reg [FOLD-3 SEED-7145] LogLoss score: 0.03437\n",
      "Base MODEL hgbc_class [FOLD-3 SEED-7145] LogLoss score: 0.03240\n",
      "Base MODEL lr_class [FOLD-3 SEED-7145] LogLoss score: 0.04746\n",
      "Base MODEL rf_class [FOLD-3 SEED-7145] LogLoss score: 0.03615\n",
      "Base MODEL xgb_class [FOLD-4 SEED-7145] LogLoss score: 0.02828\n",
      "Base MODEL cat_class [FOLD-4 SEED-7145] LogLoss score: 0.02805\n",
      "Base MODEL xgb_reg [FOLD-4 SEED-7145] LogLoss score: 0.02858\n",
      "Base MODEL cat_reg [FOLD-4 SEED-7145] LogLoss score: 0.04125\n",
      "Base MODEL hgbc_class [FOLD-4 SEED-7145] LogLoss score: 0.02891\n",
      "Base MODEL lr_class [FOLD-4 SEED-7145] LogLoss score: 0.03623\n",
      "Base MODEL rf_class [FOLD-4 SEED-7145] LogLoss score: 0.03228\n",
      "Base MODEL xgb_class [FOLD-5 SEED-7145] LogLoss score: 0.03033\n",
      "Base MODEL cat_class [FOLD-5 SEED-7145] LogLoss score: 0.03051\n",
      "Base MODEL xgb_reg [FOLD-5 SEED-7145] LogLoss score: 0.03070\n",
      "Base MODEL cat_reg [FOLD-5 SEED-7145] LogLoss score: 0.03629\n",
      "Base MODEL hgbc_class [FOLD-5 SEED-7145] LogLoss score: 0.03154\n",
      "Base MODEL lr_class [FOLD-5 SEED-7145] LogLoss score: 0.03759\n",
      "Base MODEL rf_class [FOLD-5 SEED-7145] LogLoss score: 0.03492\n",
      "Base MODEL xgb_class [FOLD-6 SEED-7145] LogLoss score: 0.03350\n",
      "Base MODEL cat_class [FOLD-6 SEED-7145] LogLoss score: 0.03332\n",
      "Base MODEL xgb_reg [FOLD-6 SEED-7145] LogLoss score: 0.03373\n",
      "Base MODEL cat_reg [FOLD-6 SEED-7145] LogLoss score: 0.04099\n",
      "Base MODEL hgbc_class [FOLD-6 SEED-7145] LogLoss score: 0.03489\n",
      "Base MODEL lr_class [FOLD-6 SEED-7145] LogLoss score: 0.05220\n",
      "Base MODEL rf_class [FOLD-6 SEED-7145] LogLoss score: 0.03709\n",
      "Base MODEL xgb_class [FOLD-7 SEED-7145] LogLoss score: 0.02564\n",
      "Base MODEL cat_class [FOLD-7 SEED-7145] LogLoss score: 0.02515\n",
      "Base MODEL xgb_reg [FOLD-7 SEED-7145] LogLoss score: 0.02589\n",
      "Base MODEL cat_reg [FOLD-7 SEED-7145] LogLoss score: 0.03170\n",
      "Base MODEL hgbc_class [FOLD-7 SEED-7145] LogLoss score: 0.02649\n",
      "Base MODEL lr_class [FOLD-7 SEED-7145] LogLoss score: 0.04309\n",
      "Base MODEL rf_class [FOLD-7 SEED-7145] LogLoss score: 0.03043\n",
      "Base MODEL xgb_class [FOLD-8 SEED-7145] LogLoss score: 0.02947\n",
      "Base MODEL cat_class [FOLD-8 SEED-7145] LogLoss score: 0.02983\n",
      "Base MODEL xgb_reg [FOLD-8 SEED-7145] LogLoss score: 0.02945\n",
      "Base MODEL cat_reg [FOLD-8 SEED-7145] LogLoss score: 0.03303\n",
      "Base MODEL hgbc_class [FOLD-8 SEED-7145] LogLoss score: 0.03104\n",
      "Base MODEL lr_class [FOLD-8 SEED-7145] LogLoss score: 0.03529\n",
      "Base MODEL rf_class [FOLD-8 SEED-7145] LogLoss score: 0.03325\n",
      "Base MODEL xgb_class [FOLD-9 SEED-7145] LogLoss score: 0.03466\n",
      "Base MODEL cat_class [FOLD-9 SEED-7145] LogLoss score: 0.03486\n",
      "Base MODEL xgb_reg [FOLD-9 SEED-7145] LogLoss score: 0.03489\n",
      "Base MODEL cat_reg [FOLD-9 SEED-7145] LogLoss score: 0.03562\n",
      "Base MODEL hgbc_class [FOLD-9 SEED-7145] LogLoss score: 0.03646\n",
      "Base MODEL lr_class [FOLD-9 SEED-7145] LogLoss score: 0.05494\n",
      "Base MODEL rf_class [FOLD-9 SEED-7145] LogLoss score: 0.03889\n",
      "Base MODEL xgb_class [FOLD-0 SEED-7237] LogLoss score: 0.03421\n",
      "Base MODEL cat_class [FOLD-0 SEED-7237] LogLoss score: 0.03344\n",
      "Base MODEL xgb_reg [FOLD-0 SEED-7237] LogLoss score: 0.03513\n",
      "Base MODEL cat_reg [FOLD-0 SEED-7237] LogLoss score: 0.04208\n",
      "Base MODEL hgbc_class [FOLD-0 SEED-7237] LogLoss score: 0.03484\n",
      "Base MODEL lr_class [FOLD-0 SEED-7237] LogLoss score: 0.06065\n",
      "Base MODEL rf_class [FOLD-0 SEED-7237] LogLoss score: 0.03891\n",
      "Base MODEL xgb_class [FOLD-1 SEED-7237] LogLoss score: 0.03545\n",
      "Base MODEL cat_class [FOLD-1 SEED-7237] LogLoss score: 0.03504\n",
      "Base MODEL xgb_reg [FOLD-1 SEED-7237] LogLoss score: 0.03596\n",
      "Base MODEL cat_reg [FOLD-1 SEED-7237] LogLoss score: 0.03612\n",
      "Base MODEL hgbc_class [FOLD-1 SEED-7237] LogLoss score: 0.03644\n",
      "Base MODEL lr_class [FOLD-1 SEED-7237] LogLoss score: 0.05461\n",
      "Base MODEL rf_class [FOLD-1 SEED-7237] LogLoss score: 0.03952\n",
      "Base MODEL xgb_class [FOLD-2 SEED-7237] LogLoss score: 0.02736\n",
      "Base MODEL cat_class [FOLD-2 SEED-7237] LogLoss score: 0.02743\n",
      "Base MODEL xgb_reg [FOLD-2 SEED-7237] LogLoss score: 0.02742\n",
      "Base MODEL cat_reg [FOLD-2 SEED-7237] LogLoss score: 0.03381\n",
      "Base MODEL hgbc_class [FOLD-2 SEED-7237] LogLoss score: 0.02891\n",
      "Base MODEL lr_class [FOLD-2 SEED-7237] LogLoss score: 0.03834\n",
      "Base MODEL rf_class [FOLD-2 SEED-7237] LogLoss score: 0.03025\n",
      "Base MODEL xgb_class [FOLD-3 SEED-7237] LogLoss score: 0.03208\n",
      "Base MODEL cat_class [FOLD-3 SEED-7237] LogLoss score: 0.03185\n",
      "Base MODEL xgb_reg [FOLD-3 SEED-7237] LogLoss score: 0.03181\n",
      "Base MODEL cat_reg [FOLD-3 SEED-7237] LogLoss score: 0.03518\n",
      "Base MODEL hgbc_class [FOLD-3 SEED-7237] LogLoss score: 0.03549\n",
      "Base MODEL lr_class [FOLD-3 SEED-7237] LogLoss score: 0.04933\n",
      "Base MODEL rf_class [FOLD-3 SEED-7237] LogLoss score: 0.03601\n",
      "Base MODEL xgb_class [FOLD-4 SEED-7237] LogLoss score: 0.02737\n",
      "Base MODEL cat_class [FOLD-4 SEED-7237] LogLoss score: 0.02857\n",
      "Base MODEL xgb_reg [FOLD-4 SEED-7237] LogLoss score: 0.02792\n",
      "Base MODEL cat_reg [FOLD-4 SEED-7237] LogLoss score: 0.03586\n",
      "Base MODEL hgbc_class [FOLD-4 SEED-7237] LogLoss score: 0.02884\n",
      "Base MODEL lr_class [FOLD-4 SEED-7237] LogLoss score: 0.04342\n",
      "Base MODEL rf_class [FOLD-4 SEED-7237] LogLoss score: 0.03199\n",
      "Base MODEL xgb_class [FOLD-5 SEED-7237] LogLoss score: 0.03227\n",
      "Base MODEL cat_class [FOLD-5 SEED-7237] LogLoss score: 0.03205\n",
      "Base MODEL xgb_reg [FOLD-5 SEED-7237] LogLoss score: 0.03204\n",
      "Base MODEL cat_reg [FOLD-5 SEED-7237] LogLoss score: 0.04503\n",
      "Base MODEL hgbc_class [FOLD-5 SEED-7237] LogLoss score: 0.03490\n",
      "Base MODEL lr_class [FOLD-5 SEED-7237] LogLoss score: 0.04658\n",
      "Base MODEL rf_class [FOLD-5 SEED-7237] LogLoss score: 0.03560\n",
      "Base MODEL xgb_class [FOLD-6 SEED-7237] LogLoss score: 0.03618\n",
      "Base MODEL cat_class [FOLD-6 SEED-7237] LogLoss score: 0.03581\n",
      "Base MODEL xgb_reg [FOLD-6 SEED-7237] LogLoss score: 0.03559\n",
      "Base MODEL cat_reg [FOLD-6 SEED-7237] LogLoss score: 0.03921\n",
      "Base MODEL hgbc_class [FOLD-6 SEED-7237] LogLoss score: 0.03712\n",
      "Base MODEL lr_class [FOLD-6 SEED-7237] LogLoss score: 0.04968\n",
      "Base MODEL rf_class [FOLD-6 SEED-7237] LogLoss score: 0.04023\n",
      "Base MODEL xgb_class [FOLD-7 SEED-7237] LogLoss score: 0.02540\n",
      "Base MODEL cat_class [FOLD-7 SEED-7237] LogLoss score: 0.02487\n",
      "Base MODEL xgb_reg [FOLD-7 SEED-7237] LogLoss score: 0.02465\n",
      "Base MODEL cat_reg [FOLD-7 SEED-7237] LogLoss score: 0.03590\n",
      "Base MODEL hgbc_class [FOLD-7 SEED-7237] LogLoss score: 0.02633\n",
      "Base MODEL lr_class [FOLD-7 SEED-7237] LogLoss score: 0.04279\n",
      "Base MODEL rf_class [FOLD-7 SEED-7237] LogLoss score: 0.02922\n",
      "Base MODEL xgb_class [FOLD-8 SEED-7237] LogLoss score: 0.03163\n",
      "Base MODEL cat_class [FOLD-8 SEED-7237] LogLoss score: 0.03036\n",
      "Base MODEL xgb_reg [FOLD-8 SEED-7237] LogLoss score: 0.03095\n",
      "Base MODEL cat_reg [FOLD-8 SEED-7237] LogLoss score: 0.03390\n",
      "Base MODEL hgbc_class [FOLD-8 SEED-7237] LogLoss score: 0.03169\n",
      "Base MODEL lr_class [FOLD-8 SEED-7237] LogLoss score: 0.06325\n",
      "Base MODEL rf_class [FOLD-8 SEED-7237] LogLoss score: 0.03556\n",
      "Base MODEL xgb_class [FOLD-9 SEED-7237] LogLoss score: 0.03001\n",
      "Base MODEL cat_class [FOLD-9 SEED-7237] LogLoss score: 0.02964\n",
      "Base MODEL xgb_reg [FOLD-9 SEED-7237] LogLoss score: 0.02991\n",
      "Base MODEL cat_reg [FOLD-9 SEED-7237] LogLoss score: 0.03539\n",
      "Base MODEL hgbc_class [FOLD-9 SEED-7237] LogLoss score: 0.03081\n",
      "Base MODEL lr_class [FOLD-9 SEED-7237] LogLoss score: 0.04318\n",
      "Base MODEL rf_class [FOLD-9 SEED-7237] LogLoss score: 0.03328\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "432"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# base model and evaluating using stacking class\n",
    "base_model_num = StackingClassifier().len_base\n",
    "\n",
    "# initialize empty list and arrays for storing model objects\n",
    "# best iterations, scores, and predictions\n",
    "models =[]\n",
    "best_iterations = []\n",
    "scores = []\n",
    "oof_predss = np.zeros((X_train.shape[0], base_model_num))\n",
    "test_predss = np.zeros((X_test.shape[0], base_model_num))\n",
    "\n",
    "# loop over each split and train base models using training data and\n",
    "# evaluate on validation data\n",
    "for i, (X_train_, X_val, y_train_, y_val) in enumerate(splitter.split_data(X_train, y_train, random_state_list=random_state_list)):\n",
    "    n = i % n_splits\n",
    "    m = i // n_splits\n",
    "\n",
    "    stacking_clf = StackingClassifier(n_estimators, device, random_state)\n",
    "    base_models = stacking_clf.base_models\n",
    "\n",
    "    # initialize lists to store oof and predictions for each base model\n",
    "    oof_preds = []\n",
    "    test_preds = []\n",
    "\n",
    "    for name, model in base_models.items():\n",
    "        if name in ['rf_class', 'hgbc_class', 'lr_class']:\n",
    "            model.fit(X_train_, y_train_)\n",
    "        else:\n",
    "            model.fit(X_train_, y_train_, eval_set=[(X_val, y_val)], early_stopping_rounds=early_stopping_rounds, verbose=verbose)\n",
    "        \n",
    "        if 'class' in name:\n",
    "            y_val_pred = model.predict_proba(X_val)[:, 1]\n",
    "            test_pred = model.predict_proba(X_test)[:, 1]\n",
    "        else:\n",
    "            y_val_pred = model.predict(X_val)\n",
    "            test_pred = model.predict(X_test)\n",
    "        \n",
    "        score = log_loss(y_val, y_val_pred)\n",
    "        print(f'Base MODEL {name} [FOLD-{n} SEED-{random_state_list[m]}] LogLoss score: {score:.5f}')\n",
    "\n",
    "        oof_preds.append(y_val_pred)\n",
    "        test_preds.append(test_pred)\n",
    "\n",
    "    # stack oof and test preds horizontally for each base model and store in oof_predss and test_predss respectively\n",
    "    oof_preds = np.column_stack(oof_preds)\n",
    "    oof_predss[X_val.index] = oof_preds\n",
    "    test_preds = np.column_stack(test_preds)\n",
    "    test_predss += test_preds / (n_splits * len(random_state_list))\n",
    "\n",
    "    i += 1\n",
    "\n",
    "gc.collect() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [105807, 117564]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/juandiegogallegoquiceno/Desktop/PersonalProjects/Learning/kaggle_playground_s3e10/notebooks/1-train.ipynb Cell 12\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/juandiegogallegoquiceno/Desktop/PersonalProjects/Learning/kaggle_playground_s3e10/notebooks/1-train.ipynb#X15sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m weights \u001b[39m=\u001b[39m []\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/juandiegogallegoquiceno/Desktop/PersonalProjects/Learning/kaggle_playground_s3e10/notebooks/1-train.ipynb#X15sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39m# evaluate on validation data and store predictions on test\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/juandiegogallegoquiceno/Desktop/PersonalProjects/Learning/kaggle_playground_s3e10/notebooks/1-train.ipynb#X15sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mfor\u001b[39;00m i, (X_train_, X_val, y_train_, y_val) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(splitter\u001b[39m.\u001b[39msplit_data(X_train, y_train, random_state_list\u001b[39m=\u001b[39mrandom_state_list)):\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/juandiegogallegoquiceno/Desktop/PersonalProjects/Learning/kaggle_playground_s3e10/notebooks/1-train.ipynb#X15sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     n \u001b[39m=\u001b[39m i \u001b[39m%\u001b[39m n_splits\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/juandiegogallegoquiceno/Desktop/PersonalProjects/Learning/kaggle_playground_s3e10/notebooks/1-train.ipynb#X15sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     m \u001b[39m=\u001b[39m i \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m n_splits\n",
      "\u001b[1;32m/Users/juandiegogallegoquiceno/Desktop/PersonalProjects/Learning/kaggle_playground_s3e10/notebooks/1-train.ipynb Cell 12\u001b[0m in \u001b[0;36mSplitter.split_data\u001b[0;34m(self, X, y, random_state_list)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/juandiegogallegoquiceno/Desktop/PersonalProjects/Learning/kaggle_playground_s3e10/notebooks/1-train.ipynb#X15sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mfor\u001b[39;00m random_state \u001b[39min\u001b[39;00m random_state_list:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/juandiegogallegoquiceno/Desktop/PersonalProjects/Learning/kaggle_playground_s3e10/notebooks/1-train.ipynb#X15sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     kf \u001b[39m=\u001b[39m StratifiedKFold(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/juandiegogallegoquiceno/Desktop/PersonalProjects/Learning/kaggle_playground_s3e10/notebooks/1-train.ipynb#X15sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m         n_splits\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_splits,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/juandiegogallegoquiceno/Desktop/PersonalProjects/Learning/kaggle_playground_s3e10/notebooks/1-train.ipynb#X15sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m         random_state\u001b[39m=\u001b[39mrandom_state,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/juandiegogallegoquiceno/Desktop/PersonalProjects/Learning/kaggle_playground_s3e10/notebooks/1-train.ipynb#X15sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m         shuffle\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/juandiegogallegoquiceno/Desktop/PersonalProjects/Learning/kaggle_playground_s3e10/notebooks/1-train.ipynb#X15sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     )\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/juandiegogallegoquiceno/Desktop/PersonalProjects/Learning/kaggle_playground_s3e10/notebooks/1-train.ipynb#X15sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     \u001b[39mfor\u001b[39;00m train_index, val_index \u001b[39min\u001b[39;00m kf\u001b[39m.\u001b[39msplit(X, y):\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/juandiegogallegoquiceno/Desktop/PersonalProjects/Learning/kaggle_playground_s3e10/notebooks/1-train.ipynb#X15sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m         X_train, X_val \u001b[39m=\u001b[39m X\u001b[39m.\u001b[39miloc[train_index], X\u001b[39m.\u001b[39miloc[val_index]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/juandiegogallegoquiceno/Desktop/PersonalProjects/Learning/kaggle_playground_s3e10/notebooks/1-train.ipynb#X15sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m         y_train, y_val \u001b[39m=\u001b[39m y\u001b[39m.\u001b[39miloc[train_index], y\u001b[39m.\u001b[39miloc[val_index]\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/ccFraud-bz6UI_fL/lib/python3.9/site-packages/sklearn/model_selection/_split.py:330\u001b[0m, in \u001b[0;36m_BaseKFold.split\u001b[0;34m(self, X, y, groups)\u001b[0m\n\u001b[1;32m    306\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msplit\u001b[39m(\u001b[39mself\u001b[39m, X, y\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, groups\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    307\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Generate indices to split data into training and test set.\u001b[39;00m\n\u001b[1;32m    308\u001b[0m \n\u001b[1;32m    309\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    328\u001b[0m \u001b[39m        The testing set indices for that split.\u001b[39;00m\n\u001b[1;32m    329\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 330\u001b[0m     X, y, groups \u001b[39m=\u001b[39m indexable(X, y, groups)\n\u001b[1;32m    331\u001b[0m     n_samples \u001b[39m=\u001b[39m _num_samples(X)\n\u001b[1;32m    332\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_splits \u001b[39m>\u001b[39m n_samples:\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/ccFraud-bz6UI_fL/lib/python3.9/site-packages/sklearn/utils/validation.py:433\u001b[0m, in \u001b[0;36mindexable\u001b[0;34m(*iterables)\u001b[0m\n\u001b[1;32m    414\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Make arrays indexable for cross-validation.\u001b[39;00m\n\u001b[1;32m    415\u001b[0m \n\u001b[1;32m    416\u001b[0m \u001b[39mChecks consistent length, passes through None, and ensures that everything\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    429\u001b[0m \u001b[39m    sparse matrix, or dataframe) or `None`.\u001b[39;00m\n\u001b[1;32m    430\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    432\u001b[0m result \u001b[39m=\u001b[39m [_make_indexable(X) \u001b[39mfor\u001b[39;00m X \u001b[39min\u001b[39;00m iterables]\n\u001b[0;32m--> 433\u001b[0m check_consistent_length(\u001b[39m*\u001b[39;49mresult)\n\u001b[1;32m    434\u001b[0m \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/ccFraud-bz6UI_fL/lib/python3.9/site-packages/sklearn/utils/validation.py:387\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    385\u001b[0m uniques \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39munique(lengths)\n\u001b[1;32m    386\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(uniques) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m--> 387\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    388\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    389\u001b[0m         \u001b[39m%\u001b[39m [\u001b[39mint\u001b[39m(l) \u001b[39mfor\u001b[39;00m l \u001b[39min\u001b[39;00m lengths]\n\u001b[1;32m    390\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [105807, 117564]"
     ]
    }
   ],
   "source": [
    "# stacking model with bas and meta models\n",
    "meta_test_predss = np.zeros(X_test.shape[0])\n",
    "ensemble_score = []\n",
    "weights = []\n",
    "\n",
    "# evaluate on validation data and store predictions on test\n",
    "for i, (X_train_, X_val, y_train_, y_val) in enumerate(splitter.split_data(X_train, y_train, random_state_list=random_state_list)):\n",
    "    n = i % n_splits\n",
    "    m = i // n_splits\n",
    "\n",
    "    train_index, val_index = X_train_.index, X_val.index\n",
    "\n",
    "    # use predictions from base models as input features\n",
    "    X_train_ = oof_predss[train_index]\n",
    "    X_val = oof_predss[val_index]\n",
    "\n",
    "    # get a set of base models and meta model using the get_model_function\n",
    "    stacking_clf = StackingClassifier(n_estimators, device, random_state)\n",
    "    meta_models = stacking_clf.meta_models\n",
    "\n",
    "    # initialize lists to store oof and test predictions for each base model\n",
    "    oof_preds = []\n",
    "    test_preds = []\n",
    "\n",
    "    # loop over each base model and fit on train data, eval on validation\n",
    "    for name, model in meta_models.items():\n",
    "        if name in ['rf_class', 'hgbc_class', 'lr_class']:\n",
    "            model.fit(X_train_, y_train_)\n",
    "        else:\n",
    "            model.fit(X_train_, y_train_, eval_set=[(X_val, y_val)], early_stopping_rounds=early_stopping_rounds, verbose=verbose)\n",
    "\n",
    "        y_val_pred = model.predict_proba(X_val)[:, 1]\n",
    "        test_pred = model.predict_proba(test_predss)[:, 1]\n",
    "        score = log_loss(y_val, y_val_pred)\n",
    "        print(f'Meta MODEL {name} [FOLD-{n} SEED-{random_state_list[m]}] LogLoss score: {score:.5f}')\n",
    "\n",
    "        oof_preds.append(y_val_pred)\n",
    "        test_preds.append(test_pred)\n",
    "\n",
    "    # use optuna to find the best ensemble weights\n",
    "    optweights = OptunaWeights(random_state=random_state)\n",
    "    y_val_pred = optweights.fit_predict(y_val.values, oof_preds)\n",
    "    score = log_loss(y_val, y_val_pred)\n",
    "    print(f'Ensemble MODEL [FOLD-{n} SEED-{random_state_list[m]}] LogLoss score {score:.5f}')\n",
    "\n",
    "    ensemble_score.append(score)\n",
    "    weights.append(optweights.weights)\n",
    "    meta_test_predss += optweights.predict(test_preds) / (n_splits * len(random_state_list))\n",
    "\n",
    "    i += 1\n",
    "\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ccFraud-bz6UI_fL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
